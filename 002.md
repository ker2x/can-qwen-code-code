# CartPole Reinforcement Learning with Vectorized Environments

This Python script demonstrates reinforcement learning using OpenAI Gym's CartPole environment with vectorized environments. The agent learns to balance a pole on a cart by taking actions based on the current state.

## Environment Setup

The script creates 2 parallel CartPole environments using `gym.make_vec()` with:
- `vectorization_mode='sync'` for synchronous execution
- `render_mode='human'` to visualize the simulations
- `CartPole-v1` environment which represents the classic control problem

## Training Process

The training loop runs for 500 episodes with a maximum of 200 steps per episode. Each episode:
1. Resets all environments to their initial states
2. Takes random actions for all environments simultaneously
3. Observes the next states and rewards
4. Accumulates total rewards across all environments
5. Continues until an episode terminates or truncates

## Neural Network Structure

The agent's model parameters are represented by a neural network with:
- 4 input neurons (CartPole state variables: cart position, cart velocity, pole angle, pole angular velocity)
- 64 neurons in first hidden layer
- 64 neurons in second hidden layer
- 2 output neurons (action probabilities: push left or push right)

Total trainable parameters: ~8,500

## Expected Performance

CartPole is typically considered solved when the agent achieves an average reward of 200+ over 100 consecutive episodes. The script prints progress every 50 episodes to monitor learning.

## Key Features

- Uses vectorized environments for efficient parallel execution
- Demonstrates how model parameters are represented in neural network structure
- Shows the relationship between environment states, actions, and rewards
- Provides visualization of the learning process through rendered windows
